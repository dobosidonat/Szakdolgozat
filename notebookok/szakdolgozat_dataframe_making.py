# -*- coding: utf-8 -*-
"""Szakdolgozat_Dataframe_making.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-p2miCwNfCmc7sM3pqo_UkJ3BND8di34

#Szükséges könyvtárak beimportálása
"""

import requests

from bs4 import BeautifulSoup

import pandas as pd

"""#Adatgyűjtés (webscrapping)

##Adatgyűjtés előkészítése

*   Szükséges könyvtárak beimportálása
*   Csapatok URL címeinek előkészítése és olyan állapotba hozása, hogy a program tudjon navigálni rajtuk
*   Szükséges táblák adatainak beimportálása, azok mergelése
"""

!pip install requests

url_stats = "https://fbref.com/en/comps/13/2022-2023/2022-2023-Ligue-1-Stats"

data = requests.get(url_stats)

soup = BeautifulSoup(data.text)
BLstats_table = soup.select('table.stats_table')[0]

#a tag-ek megtalálása (csapatok linkjei)
links = BLstats_table.find_all('a')

#a tag-ek értékeinek megtalálása
links = [l.get("href") for l in links]

#csak a csapatok a tageinek megtartása, így csak a csapatok linkjei maradnak meg
links = [l for l in links if '/squads/' in l]

#linkek eleje lemaradt, így kiegészítődnek
team_urls = [f"https://fbref.com{l}" for l in links]

team_url=team_urls[0]
data=requests.get(team_url)

team_urls

"""##Bundesliga adatok"""

# Az url-címen levő táblák caption-jeit nézi meg. Ha egyezést talál (match-el) az egyik tábla captionje a beírttal akkor leszedi annak az adatait.
players = pd.read_html(data.text, match="Standard Stats ")[0]

# Ez eltüntette a fejléc felesleges elemeit, ezzel is javítva a táblázat kinézetét.
players.columns=players.columns.droplevel()

players.head()

goalkeeping = pd.read_html(data.text, match="Goalkeeping ")[0]
goalkeeping.columns=goalkeeping.columns.droplevel()

shooting = pd.read_html(data.text, match="Shooting ")[0]
shooting.columns=shooting.columns.droplevel()

passing = pd.read_html(data.text, match="Passing ")[0]
passing.columns=passing.columns.droplevel()

defensive_actions = pd.read_html(data.text, match="Defensive Actions ")[0]
defensive_actions.columns=defensive_actions.columns.droplevel()

miscellaneous_stats = pd.read_html(data.text, match="Miscellaneous Stats ")[0]
miscellaneous_stats.columns=miscellaneous_stats.columns.droplevel()

merged_df = players.merge(goalkeeping[['Player','GA','SoTA','Saves','Save%','CS','CS%']], on='Player', how='outer')
merged_df = players.merge(shooting[['Player','Sh','SoT','SoT%','G/Sh','Dist']], on='Player', how='outer')
merged_df = merged_df.merge(passing[['Player','Cmp','Att','Cmp%','TotDist','PrgDist','xA']], on='Player', how='outer')
merged_df = merged_df.merge(defensive_actions[['Player','Tkl','TklW','Tkl%','Int','Tkl+Int','Clr','Err']], on='Player', how='outer')
merged_df = merged_df.merge(miscellaneous_stats[['Player','Won','Lost','Won%']], on='Player', how='outer')

merged_df

"""##Minden versenysorozat"""

soup = BeautifulSoup(data.text)
links = soup.find_all('a')
links = [l.get("href") for l in links]
links = [l for l in links if l and 'all_comps/Paris-Saint-Germain-Stats-All-Competitions' in l]

links

data = requests.get(f"https://fbref.com{links[0]}")

all_comps = pd.read_html(data.text, match="Standard Stats")[0]

all_comps.columns=all_comps.columns.droplevel()

all_comps_goalkeeping = pd.read_html(data.text, match="Goalkeeping ")[0]
all_comps_goalkeeping.columns=all_comps_goalkeeping.columns.droplevel()

all_comps_shooting = pd.read_html(data.text, match="Shooting ")[0]
all_comps_shooting.columns=all_comps_shooting.columns.droplevel()

all_comps_passing = pd.read_html(data.text, match="Passing ")[0]
all_comps_passing.columns=all_comps_passing.columns.droplevel()

all_comps_defensive_actions = pd.read_html(data.text, match="Defensive Actions ")[0]
all_comps_defensive_actions.columns=all_comps_defensive_actions.columns.droplevel()

all_comps_miscellaneous_stats = pd.read_html(data.text, match="Miscellaneous Stats ")[0]
all_comps_miscellaneous_stats.columns=all_comps_miscellaneous_stats.columns.droplevel()

all_comps_merged_df = all_comps.merge(all_comps_goalkeeping[['Player','GA','SoTA','Saves','Save%','CS','CS%']], on='Player', how='outer')
all_comps_merged_df = all_comps_merged_df.merge(all_comps_shooting[['Player','Sh','SoT','SoT%','G/Sh','Dist']], on='Player', how='outer')
all_comps_merged_df = all_comps_merged_df.merge(all_comps_passing[['Player','Cmp','Att','Cmp%','TotDist','PrgDist','xA']], on='Player', how='outer')
all_comps_merged_df = all_comps_merged_df.merge(all_comps_defensive_actions[['Player','Tkl','TklW','Tkl%','Int','Tkl+Int','Clr','Err']], on='Player', how='outer')
all_comps_merged_df = all_comps_merged_df.merge(all_comps_miscellaneous_stats[['Player','Won','Lost','Won%']], on='Player', how='outer')

all_comps_merged_df

"""##Adatgyűjtés visszamenőleg valamint azok mergelése dupla for ciklussal, valamint a kapott adatok konkatanálása egy dataframe-mé

###Bundesliga
"""

import time
url_stats = "https://fbref.com/en/comps/13/2022-2023/2022-2023-Ligue-1-Stats"
statsBL = []
for year in range(2022, 2017, -1):
    url_stats = f"https://fbref.com/en/comps/13/{year}-{year+1}/" \
                  f"{year}-{year+1}-Ligue-1-Stats"
    data = requests.get(url_stats)
    soup = BeautifulSoup(data.text)
    BLstats_table = soup.select('table.stats_table')[0]

    links = [l.get("href") for l in BLstats_table.find_all('a')]
    links = [l for l in links if '/squads/' in l]
    team_urls = [f"https://fbref.com{l}" for l in links]

    previous_season = soup.select("a.prev")[0].get("href")
    url_stats = f"https://fbref.com/{previous_season}"

    for team_url in team_urls:
      teams = team_url.split("/")[-1].replace("-Stats","")
      team_name = team_url.split("/")[-1].replace("-Stats","").replace("-"," ")
      competition= url_stats.split("/")[-1].split("-")[2].replace("-Stats","")
      season=f"{year}-{year+1}"

      data = requests.get(team_url)
      players = pd.read_html(data.text, match="Standard Stats ")[0]
      players.columns=players.columns.droplevel()
      goalkeeping = pd.read_html(data.text, match="Goalkeeping ")[0]
      goalkeeping.columns=goalkeeping.columns.droplevel()
      shooting = pd.read_html(data.text, match="Shooting ")[0]
      shooting.columns=shooting.columns.droplevel()
      passing = pd.read_html(data.text, match="Passing ")[0]
      passing.columns=passing.columns.droplevel()
      defensive_actions = pd.read_html(data.text, match="Defensive Actions ")[0]
      defensive_actions.columns=defensive_actions.columns.droplevel()
      miscellaneous_stats = pd.read_html(data.text, match="Miscellaneous Stats ")[0]
      miscellaneous_stats.columns=miscellaneous_stats.columns.droplevel()

      merged_df = players.merge(goalkeeping[['Player','GA','SoTA','Saves','Save%','CS','CS%']], on='Player', how='outer')
      merged_df = merged_df.merge(shooting[['Player','Sh','SoT','SoT%','G/Sh','Dist']], on='Player', how='outer')
      merged_df = merged_df.merge(passing[['Player','Cmp','Att','Cmp%','TotDist','PrgDist','xA']], on='Player', how='outer')
      merged_df = merged_df.merge(defensive_actions[['Player','Tkl','TklW','Tkl%','Int','Tkl+Int','Clr','Err']], on='Player', how='outer')
      merged_df = merged_df.merge(miscellaneous_stats[['Player','Won','Lost','Won%']], on='Player', how='outer')


      merged_df["Season"] = season
      merged_df["Team"] = team_name
      merged_df["Competition"]=competition

      statsBL.append(merged_df)
      time.sleep(1)

BL=pd.concat(statsBL)

BL

BL["Competition"]="Ligue 1"
BL

BL.to_csv("Ligue1.csv")

"""###All Competitions"""

import time
url_stats = "https://fbref.com/en/comps/13/2022-2023/2022-2023-Ligue-1-Stats"
statsBL_AllComp = []
for year in range(2022, 2017, -1):
    url_stats = f"https://fbref.com/en/comps/13/{year}-{year+1}/" \
                  f"{year}-{year+1}-Ligue-1-Stats"
    data = requests.get(url_stats)
    soup = BeautifulSoup(data.text)
    BLstats_table = soup.select('table.stats_table')[0]

    links = [l.get("href") for l in BLstats_table.find_all('a')]
    links = [l for l in links if '/squads/' in l]
    team_urls = [f"https://fbref.com{l}" for l in links]

    previous_season = soup.select("a.prev")[0].get("href")
    url_stats = f"https://fbref.com/{previous_season}"

    for team_url in team_urls:
      teams = team_url.split("/")[-1].replace("-Stats","")
      team_name = team_url.split("/")[-1].replace("-Stats","").replace("-"," ")
      competition= url_stats.split("/")[-1].split("-")[2].replace("-Stats","")
      season=f"{year}-{year+1}"

      data = requests.get(team_url)
      soup = BeautifulSoup(data.text)
      links = soup.find_all('a')
      links = [l.get("href") for l in links]
      links = [l for l in links if l and '/all_comps/{}-Stats-All-Competitions'.format(teams) in l]
      data = requests.get(f"https://fbref.com{links[0]}")
      all_comps = pd.read_html(data.text, match="Standard Stats")[0]
      all_comps.columns=all_comps.columns.droplevel()
      all_comps_goalkeeping = pd.read_html(data.text, match="Goalkeeping ")[0]
      all_comps_goalkeeping.columns=all_comps_goalkeeping.columns.droplevel()
      all_comps_shooting = pd.read_html(data.text, match="Shooting ")[0]
      all_comps_shooting.columns=all_comps_shooting.columns.droplevel()
      all_comps_passing = pd.read_html(data.text, match="Passing ")[0]
      all_comps_passing.columns=all_comps_passing.columns.droplevel()
      all_comps_defensive_actions = pd.read_html(data.text, match="Defensive Actions ")[0]
      all_comps_defensive_actions.columns=all_comps_defensive_actions.columns.droplevel()
      all_comps_miscellaneous_stats = pd.read_html(data.text, match="Miscellaneous Stats ")[0]
      all_comps_miscellaneous_stats.columns=all_comps_miscellaneous_stats.columns.droplevel()

      all_comps_merged_df = all_comps.merge(all_comps_goalkeeping[['Player','GA','SoTA','Saves','Save%','CS','CS%']], on='Player', how='outer')
      all_comps_merged_df = all_comps_merged_df.merge(all_comps_shooting[['Player','Sh','SoT','SoT%','G/Sh','Dist']], on='Player', how='outer')
      all_comps_merged_df = all_comps_merged_df.merge(all_comps_passing[['Player','Cmp','Att','Cmp%','TotDist','PrgDist','xA']], on='Player', how='outer')
      all_comps_merged_df = all_comps_merged_df.merge(all_comps_defensive_actions[['Player','Tkl','TklW','Tkl%','Int','Tkl+Int','Clr','Err']], on='Player', how='outer')
      all_comps_merged_df = all_comps_merged_df.merge(all_comps_miscellaneous_stats[['Player','Won','Lost','Won%']], on='Player', how='outer')




      all_comps_merged_df["Season"] = season
      all_comps_merged_df["Team"] = team_name
      all_comps_merged_df["Competition"]=competition

      statsBL_AllComp.append(all_comps_merged_df)
      time.sleep(1)

BLallcomp_df=pd.concat(statsBL_AllComp)

BLallcomp_df

BLallcomp_df["Competition"]="Ligue 1"
BLallcomp_df

BLallcomp_df.to_csv("Ligue1_All_Comp.csv")

"""# Adattisztítás


*   A webscrapping során kapott .csv file-ok beolvasása egy-egy új dataframebe
*   Felesleges oszlopok illetve sorok eltávolítása
*   A lecsupaszított dataframe-ek kiimportálása egy-egy új .csv file-ba

## Bundesliga adatok
"""

BL=pd.read_csv("PL.csv", index_col=0)

del BL["npxG+xAG"]
del BL["Cmp.1"]
del BL["Cmp.2"]
del BL["Cmp.3"]
del BL["Att.1"]
del BL["Att.2"]
del BL["Att.3"]
del BL["Cmp%.1"]
del BL["Cmp%.2"]
del BL["Cmp%.3"]
del BL["PrgC"]
del BL["PrgP"]
del BL["PrgR"]
del BL["Gls.1"]
del BL["Ast.1"]
del BL["G+A.1"]
del BL["G-PK.1"]
del BL["G+A-PK"]
del BL["xG.1"]
del BL["xAG.1"]
del BL["npxG+xAG.1"]
del BL["Matches"]
del BL["xG+xAG"]
del BL['npxG.1']

BL.rename(columns={"PK" : "Penalty_Kicks_Made","Save%.1" : "PK_Save%","Tkl.1" : "Challenges_Tkl" }, inplace=True)

BL=BL[BL["Player"]!="Squad Total"]

BL=BL[BL["Player"]!="Opponent Total"]

BL.to_csv("PL_vegleges.csv")

"""## Minden versenysorozat adata"""

BL_All=pd.read_csv("Ligue1_All_Comp.csv", index_col=0)

del BL_All["npxG+xAG"]
del BL_All["Cmp.1"]
del BL_All["Cmp.2"]
del BL_All["Cmp.3"]
del BL_All["Att.1"]
del BL_All["Att.2"]
del BL_All["Att.3"]
del BL_All["Cmp%.1"]
del BL_All["Cmp%.2"]
del BL_All["Cmp%.3"]
del BL_All["PrgC"]
del BL_All["PrgP"]
del BL_All["PrgR"]
del BL_All["Gls.1"]
del BL_All["Ast.1"]
del BL_All["G+A.1"]
del BL_All["G-PK.1"]
del BL_All["G+A-PK"]
del BL_All["xG.1"]
del BL_All["xAG.1"]
del BL_All["npxG+xAG.1"]
del BL_All["Matches"]
del BL_All["xG+xAG"]

BL_All.rename(columns={"PK" : "Penalty_Kicks_Made","Save%.1" : "PK_Save%","Tkl.1" : "Challenges_Tkl" }, inplace=True)

BL_All[BL_All["Player"]=="Opponent Total"]

BL_All[BL_All["Player"]=="Squad Total"]

BL_All=BL_All[BL_All["Player"]!="Opponent Total"]
BL_All=BL_All[BL_All["Player"]!="Squad Total"]

BL_All

BL_All.to_csv("Ligue1_All_vegleges.csv")

"""#Bajnoki tabellák
##Meg szeretném vizsgálni hogy egyes csapatok meglepetésszerű mélyrepülése vagy szárnyalása mennyiben köszönhető egyes kulcsjátékosainak a kiemelkedő teljesítményéhez és milyen formában valósul ez meg.
"""

url_stats = "https://fbref.com/en/comps/9/2022-2023/2022-2023-Premier-League-Stats"

data = requests.get(url_stats)

soup=BeautifulSoup(data.text,"html.parser")

stats_table = soup.select('table.stats_table')[0]

standings= pd.read_html(str(stats_table))[0]

standings

import time
url_stats = "https://fbref.com/en/comps/20/2022-2023/2022-2023-Bundesliga-Stats"
standings_table = []
for year in range(2022, 2017, -1):
    url_stats = f"https://fbref.com/en/comps/20/{year}-{year+1}/" \
                  f"{year}-{year+1}-Bundesliga-Stats"
    data = requests.get(url_stats)
    soup=BeautifulSoup(data.text,"html.parser")
    stats_table = soup.select('table.stats_table')[0]
    standings= pd.read_html(str(stats_table))[0]
    previous_season = soup.select("a.prev")[0].get("href")
    url_stats = f"https://fbref.com/{previous_season}"
    competition= url_stats.split("/")[-1].split("-")[2].replace("-Stats","")
    season=f"{year}-{year+1}"

    standings["Season"] = season
    standings["Competition"]=competition

    standings_table.append(standings)
    time.sleep(1)

df=pd.concat(standings_table)

df["Competition"]="Seria A"

del df["Pts/MP"]
del df["xG"]
del df["xGA"]
del df["xGD"]
del df["xGD/90"]
del df["Attendance"]
del df["Top Team Scorer"]
del df["Goalkeeper"]
del df["Notes"]

df

df.to_csv("Bundesliga.csv")



"""#Aranylabda győztesek
A futball világ legnagyobb presztizsű egyéni díja az Aranylabda, amelyet az adott szezon legjobb játékosa nyer el minden évben. 2018-2019 ÉS 2022-2023 között 4-szer hírdettek győztest. Meg szeretném jósolni, hogy nyerni a 2023-2024-es szezon Aranylabdáját.

##2023
"""

url_stats = "https://en.wikipedia.org/wiki/2023_Ballon_d%27Or"

data = requests.get(url_stats)

soup=BeautifulSoup(data.text,"html.parser")

ranking_table = soup.select('table.wikitable')[0]

ranking2023= pd.read_html(str(ranking_table))[0]

ranking2023["Points%"]=ranking2023['Points']/ranking2023['Points'].sum()

ranking2023

"""##2022"""

url_stats = "https://en.wikipedia.org/wiki/2022_Ballon_d%27Or"

data = requests.get(url_stats)

soup=BeautifulSoup(data.text,"html.parser")

ranking_table = soup.select('table.wikitable')[0]

ranking2022= pd.read_html(str(ranking_table))[0]

ranking2022["Points%"]=ranking2022['Points']/ranking2022['Points'].sum()

ranking2022

"""##2021"""

url_stats = "https://en.wikipedia.org/wiki/2021_Ballon_d%27Or"

data = requests.get(url_stats)

soup=BeautifulSoup(data.text,"html.parser")

ranking_table = soup.select('table.wikitable')[0]

ranking2021= pd.read_html(str(ranking_table))[0]

ranking2021["Points%"]=ranking2021['Points']/ranking2021['Points'].sum()

ranking2021

"""##2019"""

url_stats = "https://en.wikipedia.org/wiki/2019_Ballon_d%27Or"

data = requests.get(url_stats)

soup=BeautifulSoup(data.text,"html.parser")

ranking_table = soup.select('table.wikitable')[0]

ranking2019= pd.read_html(str(ranking_table))[0]

ranking2019["Points%"]=ranking2019['Points']/ranking2019['Points'].sum()

ranking2019

"""##Dataframek formázása és egyesítése"""

top10_2023=ranking2023.nlargest(10,'Points')
top10_2023["Season"]="2022-2023"
del top10_2023['Nationality']
del top10_2023['Position']
top10_2023["Club"]=top10_2023["Club"].str.replace("Inter Miami[a]","Paris Saint-Germain", regex=False)
top10_2023.rename(columns={"Club" : "Team"}, inplace=True)
top10_2023

top10_2022=ranking2022.nlargest(10,'Points')
top10_2022["Season"]="2021-2022"
del top10_2022['Nationality']
del top10_2022['Position']
top10_2022.rename(columns={"Club" : "Team"}, inplace=True)
top10_2022

top10_2021=ranking2021.nlargest(10,'Points')
top10_2021["Season"]="2020-2021"
top10_2021.rename(columns={"Club(s)" : "Team"}, inplace=True)

top10_2021

top10_2019=ranking2019.nlargest(10,'Points')
top10_2019["Season"]="2018-2019"
top10_2019.rename(columns={"Club(s)" : "Team"}, inplace=True)
top10_2019

bd_df = pd.concat([top10_2023, top10_2022, top10_2021, top10_2019], ignore_index=True)
bd_df

bd_df.to_csv("Aranylabda.csv")